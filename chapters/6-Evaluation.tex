\chapter{Evaluation}
\label{cap:Evaluation}
This chapter will describe the evaluation testing done and aims to evaluate the signatures and rules created in this research. First the method used in the evaluation will be presented and then the evaluation results. 

\section{Evaluation Method}
During the evaluation, the same portable smart home infrastructure as used in both  Oslo and Drammen environment will be used. This is due to availability of smart homes and time for the research. The robot vacuum cleaner will be deployed in three different rooms with different setup of furniture. For each new evaluation environment the robot vacuum cleaner will be restored back to factory default, this is to ensure that configuration done in the other environments will effect the new result. 
When deployment and base configuration is done, the vacuum cleaner will do a room discovery. During this process it will create a smart home map that will be done during the tests. A predefined cleaning is configured based on this smart home map, which will be triggered in all the cleaning events. 
To make the environment more similar as a live smart home environment a new smart device will be connected to the same SSID as the robot vacuum cleaner, one for each evaluation environment. 

\begin{itemize}
    \item Evaluation environment 1: Ipad connected
    \item Evaluation environment 2: Laptop connected
    \item Evaluation environment 3: Smart phone connected
\end{itemize}

During the event triggering there will be traffic generated by the new connected device. This will generate traffic which can interfere with the event detection if not excluded in the data filtering process. 
For each environment there will be triggered one of each event. To structure this there will be triggered one event within each half an hour, it would be executed in this manner: 

\begin{itemize}
    \item Event 1: between XX:00 and XX:30
    \item Event 2: between XX:30 and XX+1:00
    \item Event 3: between XX+1:00 and XX+1:30
    \item Event 4: between XX+1:30 and XX+2:00
    \item Event 5: between XX+2:00 and XX+2:30
    \item Event 6: between XX+2:30 and XX+3:00
\end{itemize}

To make the events occur in a random sequence a script is used to shuffle the order of events. 
\begin{lstlisting}
# Python code, Application start detection
1. event_list = [scheduled_cleaning, Automated_cleaning, Application_triggered_cleaning, Application_start, Physical_triggered_cleaning, Bin_remove]
2. for three rounds do:
3.      shuffle event_list
4.      print shuffeled list
\end{lstlisting}

The different environments has physical setup according to \ref{fig:} 

\\
fig
\\

To be able to identify the communication between the smart home WAN address and the Irobot cloud service, an attacker will have to capture the DNS response for \textit{a2uowfjvhio0fa.iot.us-east-1.amazonaws.com}. This occur once a day, but to simulate this, the capturing will start 30 minutes before the first event time slot, to capture the restart traffic. Whenever the robot vacuum cleaner is rebooted it request all daily DNS messages. This DNS response IP-addresses is extracted with this logic in with python.

\begin{lstlisting}
# Python code, Application start detection
1. Fuction find_dns_response(even_capture)
2.  if a2uowfjvhio0fa.iot.us-east-1.amazonaws.com in event_capture
3.      filter = Responded Ip-addresses and dns
\end{lstlisting}


\subsection{Evaluation result Results}
This subsection will present the data processing and rule detection results. For data processing the DNS response for FQDN \textit{a2uowfjvhio0fa.iot.us-east-1.amazonaws.com}, is used in the filter. This will only include only the command and control traffic and not traffic towards 0550315.ingest.sentry.io and s3.amazoneaws.com. This could be included, but will not have any affect since the upload traffic is not used in the detection. 

The result from the randomize script and the order of events in the different evaluation environments is presented in table \ref{tab:evaleventoverview}

\begin{table}[H]
\small
\centering
\caption{Evaluation event overview}
\label{tab:evaleventoverview}
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Sequence} & \textbf{Evaluation 1}          & \textbf{Evaluation 2}          & \textbf{Evaluation 3}          \\ \hline
\textbf{1}        & Automated clean             & Application start              & Remove bin                     \\ \hline
\textbf{2}        & Application triggered clean & Scheduled cleaning             & Application triggered clean \\ \hline
\textbf{3}        & Scheduled cleaning             & Application triggered clean & Physical triggered clean    \\ \hline
\textbf{4}        & Physical triggered clean    & Physical triggered clean    & Application start              \\ \hline
\textbf{5}        & Application start              & Automated clean             & Scheduled cleaning             \\ \hline
\textbf{6}        & Bin remove                     & Bin remove                     & Automated cleaning             \\ \hline
\end{tabular}
\end{table}

Event files are created with the use of time filter, each files will be 30 minutes of duration. All the files are used as input to the detection code, created based on the analysis of this research. Scheduled cleaning and physical cleaning will not be differentiated due to lack of timestamp detection implementation. This will then be measured under overall cleaning detection. The results are presented in \ref{tab:Evaluation results}

\begin{table}[H]
\small
\centering
\caption{Evaluation results}
\label{tab:Evaluation results}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Event}        & Automated clean & Application clean & Cleaning & Application start & Bin removed \\ \hline
\textbf{Success rate} & 100\%           & 100\%             & 100\% & 100\%             & 0\%         \\ \hline
\end{tabular}
\end{table}

As presented the detection was able to identify the events \textit{Automated cleaning, Application triggered cleaning and Application start} with a 100\% success rate. It also detected 100\% on the cleaning events, which give a 50/50 probability for scheduled cleaning and physical triggered cleaning where only cleaning is detected. The Bin remove had a 0\% success rate.